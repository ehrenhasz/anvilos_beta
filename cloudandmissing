Systems Heritage and Hyper-Scale: A Technical Audit of Source Code and Emulation Layers1. Executive Summary: The Divergence of Computing AbstractionsThe trajectory of computing history is not a single linear path but a bifurcated evolution of abstraction. On one vector, we observe the "Cloud Era," a domain defined by the relentless decoupling of software from physical substrates. In this regime, the "machine" is no longer a tangible arrangement of silicon and capacitors but a transient, software-defined construct instantiated by code repositories like Moby, Kubernetes, and Firecracker. Here, the source code acts as the blueprint for the infrastructure itself, governing the orchestration of ephemeral resources that exist only for the duration of a workload. On the opposing vector lies the legacy of distinct hardware architectures—the "Hardware Era"—encompassing Lisp Machines, PA-RISC workstations, and early mobile devices. For these systems, the machine was a static, physical entity, and its preservation in the modern world relies on the rigorous, bit-precise emulation of its circuitry via software like QEMU, usim, and specialized calculator emulators.This report presents an exhaustive technical audit of the source code repositories and emulation environments that define these two distinct eras. It bridges the conceptual gap between the hyper-scale abstractions of the modern cloud—typified by WebAssembly and MicroVMs—and the bit-level reconstruction of historical architectures like the Symbolics 3600 or the HP 9000. By analyzing the structural composition of these repositories—from the Go-based orchestration logic of Kubernetes to the C-based microcode interpreters of MIT CADR emulators—we reveal a fundamental shift in how computing systems are designed, distributed, and preserved.The analysis highlights a critical dichotomy: while the Cloud Era prioritizes isolation, portability, and scalability through standardized interfaces (OCI, Wasm), the preservation era prioritizes fidelity, timing accuracy, and the resurrection of unique, non-standard behaviors. The report further identifies a converging trend where the techniques of the cloud (virtualization, JIT compilation) are increasingly employed to rescue the legacy of the past, creating a unified continuum of systems engineering.2. The Cloud Era: Infrastructure-as-Code and The Container RevolutionThe "Cloud Era" is characterized by the abstraction of the operating system and the hardware, allowing applications to run in isolated, reproducible environments. This section details the primary open-source projects that enable this abstraction, focusing on container runtimes, orchestration engines, and the architecture of the tools that build them.2.1. The Container Ecosystem: Moby and PodmanThe standardization of container technology has centered around the Open Container Initiative (OCI), yet the implementation strategies of the dominant platforms—Docker (via the Moby project) and Podman—reveal distinct architectural philosophies regarding privilege, modularity, and system integration.2.1.1. Moby (Docker): The Modular AssemblyThe Moby project serves as the upstream open-source framework for Docker, effectively acting as the "foundry" for the commercial product. It is not designed as a monolithic application but rather as a "Lego set" of interchangeable components that can be assembled into custom container systems.1 This modularity is central to understanding the modern container stack.Repository Architecture and ComponentsThe core repository, hosted at github.com/moby/moby, is primarily written in Go and serves as the integration point for various sub-components.2 It does not function alone; instead, it orchestrates a suite of specialized tools:Containerd: This is the industry-standard container runtime that manages the entire lifecycle of a container on a physical or virtual machine. It handles image transfer, storage, and container execution, effectively abstracting the host kernel's specific implementations.Runc: The low-level executor that actually interacts with the Linux kernel features (namespaces and cgroups) to create the isolated environment. Moby utilizes runc as its default executor, adhering strictly to OCI standards.BuildKit: A concurrent, cache-efficient build toolkit that powers the docker build command. It represents a significant evolution in how container images are constructed, optimizing the dependency graph of build steps to reduce assembly time.LinuxKit: A tool within the Moby project for building secure, lean, and portable Linux subsystems, further emphasizing the "assembly" nature of the project.Target Audience and PhilosophyExplicitly, the Moby project targets system integrators and infrastructure providers—those who wish to build their own container-based systems—rather than application developers looking for a ready-made commercial product.1 It provides the raw materials and the blueprints. This distinction is crucial: Docker is the product; Moby is the platform. The architecture encourages a component-based approach where "batteries are included but removable," allowing integrators to swap out orchestration or networking components as needed without reinventing the wheel.12.1.2. Podman: The Daemonless AlternativePodman (Pod Manager) represents a divergent philosophy in container management. Unlike Docker, which relies on a central, persistent daemon (dockerd) running with root privileges to manage all child containers, Podman is a daemonless engine.Architecture and Security ModelHosted at github.com/containers/podman, the project is also written in Go but fundamentally changes the process model.4 Podman uses a traditional fork/exec model for running containers, similar to how standard Linux processes are launched. This architectural decision eliminates the single point of failure associated with a central daemon and significantly improves security posture.Rootless Containers: A defining feature of Podman is its ability to run containers as a non-root user. This "rootless mode" ensures that even if a container is compromised, the attacker does not automatically gain root privileges on the host system.5Conmon: A critical but often overlooked component in the Podman stack is conmon (Container Monitor). Because there is no persistent daemon to watch over the container processes, Podman launches a small, lightweight C program called conmon for each container. conmon sits between the container runtime and the container manager, monitoring the container's exit status, managing its I/O streams, and ensuring that the container can survive even if the Podman process itself is terminated.4Build Requirements: Compiling Podman from source requires a Go toolchain (version 1.23.x or higher) and specific build tags such as selinux and seccomp to enable security features.4 The build process emphasizes the integration of Linux security modules, reflecting its enterprise-focused lineage (Red Hat).Ecosystem IntegrationPodman is designed to be a drop-in replacement for Docker, often aliasing docker commands to podman in user environments.5 It is part of a broader ecosystem under the github.com/containers organization, which includes:Buildah: A tool specialized for building OCI images without a daemon, often used in CI/CD pipelines where running a full Docker daemon is insecure or impractical.6Skopeo: A utility for inspecting and copying images between registries and storage backends without requiring the images to be pulled or loaded into a daemon first.This decoupling of "running" (Podman), "building" (Buildah), and "distributing" (Skopeo) contrasts with the monolithic approach of early Docker versions, offering a more granular and scriptable toolkit for cloud engineers.2.2. Orchestration and Infrastructure: Kubernetes and OpenStackMoving up the stack from individual container runtimes, we encounter the orchestration and infrastructure layers that manage these resources at scale.2.2.1. Kubernetes (K8s): The Operating System of the CloudKubernetes has cemented its status as the de facto operating system for cloud-native applications. Its source code repository (kubernetes/kubernetes) is a massive, complex monorepo that contains the core components: the API server, scheduler, controller manager, and kubelet.7Codebase Structure and Build SystemThe codebase is primarily Go, organized to support a vast array of pluggable components. Key architectural details include:Code Generation: Kubernetes relies heavily on automated code generation. Files such as those in .generated_files indicate the use of extensive tooling to generate OpenAPI specifications, client libraries, and deep-copy methods for API objects.7 This ensures consistency across the API surface but adds significant complexity to the build process.Staging Directory: A unique feature of the repository structure is the staging/src/k8s.io/ directory. This area acts as a "pseudo-external" repository where core components like kubectl, client-go, and api reside.8 This structure allows these components to be developed within the main repo but easily exported and consumed as standalone modules by other projects, facilitating a modular ecosystem around the core.Developer Entry Points: For developers looking to understand the CLI implementation, the entry point for kubectl commands is located in staging/src/k8s.io/kubectl/pkg/cmd, where each command (e.g., create, get) has its own implementation file (e.g., create.go).8Extensions and DerivativesThe complexity of the core Kubernetes codebase has led to the creation of lightweight distributions like K3s (k3s-io/k3s). K3s removes legacy cloud provider drivers, non-essential plugins, and alpha features to produce a single binary with a significantly smaller footprint.9 This demonstrates the adaptability of the source code—by stripping away the "bloat" of hyper-scale cloud integrations, the same architecture can be repurposed for resource-constrained edge devices.2.2.2. OpenStack: The Virtualized DatacenterWhile Kubernetes manages applications, OpenStack manages the underlying virtualized datacenter, acting as the open-source equivalent of AWS or Azure.Distributed Development ModelUnlike the monolithic Kubernetes repository, OpenStack is distributed across hundreds of distinct repositories hosted at opendev.org, with read-only mirrors on GitHub.10 This reflects its service-oriented architecture, where each component—Nova for compute, Swift for storage, Neutron for networking—is maintained as a separate project with its own release cycle.Gerrit and Python: The development workflow relies on Gerrit for code review, enforcing a rigorous peer-review process before code is merged. The vast majority of OpenStack is written in Python.10 This choice of language lowers the barrier to entry for reading the source code compared to the compiled Go of Kubernetes, but the complexity lies in the distributed interaction between the services.Dependency Management: Each project maintains a requirements.txt file, and the ecosystem relies on a synchronized set of dependencies to ensure compatibility across the diverse services.102.3. Secure Virtualization: Firecracker and gVisorAs multi-tenant workloads became the norm, the security isolation of standard containers—which share the host kernel—became insufficient for untrusted workloads. This necessity drove the development of secure virtualization technologies that blend the speed of containers with the isolation of virtual machines.2.3.1. Firecracker: The Serverless MicroVMDeveloped by AWS to power AWS Lambda and Fargate, Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel-based Virtual Machine (KVM) to create and manage microVMs.12Design for MinimalismFirecracker is purpose-built for speed and security, offering a startup time of less than 125 milliseconds and a memory footprint of less than 5 MiB.13 This performance is achieved by ruthlessly stripping away unnecessary functionality:Device Model: Firecracker eschews support for legacy devices like USB, PCI, and video controllers. Instead, it implements a minimal set of VirtIO devices (virtio-net, virtio-block) required for basic networking and storage.14 This reduction in the device model drastically lowers the attack surface.Jailer: Security is further enforced by the "Jailer," a wrapper component that applies strict sandboxing techniques—cgroups, seccomp filters, and namespaces—to the VMM process itself before it starts the guest VM. This ensures that even if the VMM is compromised, the attacker is contained.15Source Code: The project is written in Rust (firecracker-microvm/firecracker), leveraging the language's memory safety guarantees to prevent common vulnerabilities like buffer overflows. The control plane is API-driven; unlike QEMU, which is often configured via complex command-line arguments, Firecracker is controlled via a RESTful API over a Unix socket, allowing for dynamic configuration of running VMs.152.3.2. gVisor: The Application KernelGoogle's gVisor takes a fundamentally different approach to isolation. Instead of a full virtual machine, it implements a user-space kernel.16Architectural MechanismgVisor intercepts application system calls and services them in a distinct user-space process called the Sentry. The Sentry acts as a kernel for the application, implementing the Linux system call interface without allowing the application to directly interact with the host kernel.16 This creates a strong boundary where the "guest" kernel is effectively an unprivileged application running on the host.Gofer: File system access is mediated by a separate component called the Gofer, which enforces strict policy checks on file operations.17GPU Support and Trade-offs: Recent updates have added support for passing through GPU commands via nvproxy. However, the documentation explicitly notes that this weakens isolation. Because nvproxy relies on the host's NVIDIA driver—a large, complex, and opaque binary—it introduces a significant attack surface that gVisor cannot fully mitigate.18 This highlights the inherent tension between performance (hardware acceleration) and security (isolation).Build System: Reflecting its Google heritage, gVisor uses the Bazel build system. Users can build the runsc (run sandboxed container) binary, which integrates with Docker and Kubernetes as an OCI-compliant runtime, allowing users to switch between standard runc and gVisor with a simple configuration flag.173. The WebAssembly Frontier: A New Instruction Set ArchitectureWebAssembly (Wasm) has evolved far beyond its origins as a browser-based optimization layer. It has emerged as a universal binary format for cloud and edge computing—a "virtual ISA" (Instruction Set Architecture) that is platform-independent, secure by design, and polyglot. This section explores the runtimes and toolchains that enable Wasm to run outside the browser.3.1. Runtimes: Wasmtime and WasmerThe execution of Wasm binaries requires a runtime that can translate the virtual instructions into native machine code. The two dominant standalone runtimes, Wasmtime and Wasmer, approach this challenge with different architectural priorities.3.1.1. Wasmtime (Bytecode Alliance)Wasmtime is the reference implementation championed by the Bytecode Alliance, a consortium including Mozilla, Fastly, and Intel. It is designed to be a lightweight, secure, and standards-compliant runtime.Cranelift Integration: Wasmtime is built on top of Cranelift, a code generator written in Rust that translates Wasm bytecode into optimized machine code.19 Cranelift is explicitly designed for the "JIT-style" compilation needs of Wasm—fast translation with secure output—making it distinct from heavier compiler backends like LLVM.Security Model: Wasmtime implements a capability-based security model. By default, Wasm modules are sandboxed and cannot access files, the network, or environment variables. Access to these system resources must be explicitly granted via the WebAssembly System Interface (WASI) capabilities.20Codebase: The repository (bytecodealliance/wasmtime) is primarily Rust. It includes the runtime, the Cranelift compiler, and extensive bindings for embedding Wasm into languages like C, Python, and.NET.21 The build process utilizes cargo and supports features like addr2line for debugging traps by mapping addresses back to source filenames.203.1.2. Wasmer: The Universal RuntimeWasmer positions itself as the "universal" runtime, aiming to run Wasm on any device, from centralized servers to constrained IoT devices.Compiler Agnosticism: Unlike Wasmtime's tight integration with Cranelift, Wasmer supports multiple compiler backends to suit different use cases. It allows users to select between Singlepass (for extremely fast compilation, useful in blockchain contexts), Cranelift (for a balance of speed and optimization), and LLVM (for peak runtime performance at the cost of slower compilation).22WAPM and Ecosystem: Wasmer has developed a comprehensive package management system (WAPM) and tooling to "containerize" applications into Wasm modules. The source code (wasmerio/wasmer) is also Rust-based and includes the implementation of these various compiler backends.23Embeddability: A key focus for Wasmer is its "embeddability." It offers extensive bindings that allow Wasm modules to be seamlessly integrated into host environments like Go, PHP, Ruby, and even Postgres, effectively allowing these systems to execute safe, portable user code.243.2. Toolchains: WABT and BinaryenTo work with Wasm, developers rely on the WebAssembly Binary Toolkit (WABT), affectionately known as "wabbit."Conversion Utilities: WABT (webassembly/wabt) provides the essential utilities to translate between the human-readable textual representation of Wasm (.wat) and the binary format (.wasm).25Wasm2c: A particularly powerful tool within WABT is wasm2c. This utility transpiles Wasm binary files into C source code. This capability allows Wasm modules to be compiled by a standard C compiler (like GCC or Clang) and linked into projects where running a full Wasm runtime/VM would be too heavy or architecturally complex.26 This effectively turns Wasm into a portable source distribution format.Desugaring: Tools like wat-desugar help developers inspect the output of their compilers by "desugaring" high-level Wasm constructs into the raw, stack-machine instructions, aiding in debugging and performance analysis.273.3. Precursors: V8 and SpiderMonkeyIt is important to acknowledge that the high-performance techniques used in Wasm runtimes—JIT compilation, inline caching, and garbage collection—were pioneered by JavaScript engines.V8 (Google): The engine powering Chrome and Node.js. It compiles JS directly to native machine code. The source code is a massive C++ repository (chromium.googlesource.com/v8/v8.git), structured to allow embedding into any C++ application.28SpiderMonkey (Mozilla): The engine for Firefox. Its source code is embedded within the larger mozilla-central repository. It has been instrumental in the development of Rust (which was originally created to make a safer browser engine) and Wasm itself.304. The Mobile and Embedded Legacy: Reconstructing the Post-PC EraThe mobile revolution introduced architectures and operating systems that, while modern by historical standards, are already facing preservation challenges. Proprietary drivers, closed hardware specifications, and rapid obsolescence make this "legacy" difficult to archive.4.1. Android: AOSP and CuttlefishThe Android Open Source Project (AOSP) forms the foundation of the world's most popular operating system. However, the nature of its "openness" and the mechanisms for developing on it have shifted.AOSP Development ModelIn recent years (circa 2025/2026), Google has adjusted the release cadence of AOSP source code. Reports indicate a move towards "trunk stable" development, where the android-latest-release branch becomes the primary reference point, replacing the traditional model of massive, infrequent dumps of code corresponding to named releases.32 This shift aims to align the ecosystem but complicates the work of third-party ROM developers who relied on the stability of tagged releases.33Cuttlefish: The Virtual Android DeviceFor platform engineers, Cuttlefish has replaced the traditional Android Emulator as the primary validation tool. While the SDK emulator is optimized for app developers (focusing on speed and UI), Cuttlefish is designed to validate the OS framework itself with high fidelity.34Virtualization Architecture: Cuttlefish runs Android as a virtual machine on top of a Linux host, utilizing KVM and crosvm (or QEMU) for virtualization. It employs virtio-gpu to handle graphics, which allows it to run in "headless" modes on cloud servers (like Google Compute Engine) or with a local GUI.35Fidelity and Control: Because it represents the "canonical" state of AOSP, Cuttlefish guarantees full fidelity with the Android framework. The source code (google/android-cuttlefish) provides the host tools and the launch scripts (like launch_cvd) required to spin up these virtual instances. It supports creating complex test environments, such as multiple concurrent devices, which is essential for testing features like Bluetooth or Wi-Fi Direct connectivity between devices.344.2. Symbian: The Lost GiantBefore the dominance of iOS and Android, Symbian was the undisputed king of the smartphone market. Its complex, microkernel-based architecture and proprietary nature make it a significant challenge for preservation.EKA2L1: High-Level EmulationThe EKA2L1 project stands out as the leading open-source emulator for Symbian OS (specifically S60v1, v3, and v5).HLE Approach: Unlike low-level emulators that attempt to simulate the CPU cycle-by-cycle, EKA2L1 is a High-Level Emulator (HLE). It does not run the original Symbian kernel; instead, it reimplements the Symbian kernel's servers and APIs in C++17.37Server Reimplementation: The source code (EKA2L1/EKA2L1) contains reimplementations of critical system servers, such as the Window Server (wserv) and the File Server (fbs). By intercepting the API calls made by Symbian applications and handling them with native host code, EKA2L1 achieves high performance, allowing it to run complex 3D games and applications on modern Android phones and PCs.38 This approach is pragmatic: it bypasses the need for the proprietary, often undocumented, hardware drivers of the original Nokia/Sony Ericsson devices.4.3. PalmOS: The Handheld PioneerPalmOS defined the PDA era with its simplicity and "Zen of Palm" philosophy. Its architectural transition from the Motorola 68k (Dragonball) to ARM processors created a bifurcated legacy that emulators must address.Evolution of EmulatorsCopilot and POSE: The lineage begins with Copilot, the original open-source emulator, which evolved into the Palm OS Emulator (POSE). These early tools required a ROM image extracted from a physical device and focused on the Motorola 68k architecture.39Mu: A modern successor, Mu, focuses on cycle-accuracy and compatibility. Written in C, it supports both the Dragonball-based devices (OS 1-4) and the later ARM-based models (OS 5+). It emulates the Palm SD card hardware and plays sound correctly, addressing gaps in earlier emulation efforts.40CloudpilotEmu: Demonstrating the trend of "preservation via web," CloudpilotEmu is a web-based emulator derived from POSE and Mu sources. It runs entirely in the browser using WebAssembly. It employs a hybrid approach: using the POSE codebase for 68k emulation and incorporating uARM (a CPU emulator by Dmitry Grinberg) to handle the ARM-based OS 5 devices.41 This project highlights how modern web standards (Wasm) are becoming the universal museum for obsolete software.5. The Lisp Machine Era: The Hardware-Software UnificationLisp Machines represent a unique branch of computing history where the hardware was designed specifically to execute Lisp code. The architecture is tagged, stack-based, and microcoded. Since the hardware is rare, fragile, and proprietary, software emulation is the only viable path for keeping these systems alive.5.1. MIT CADR: The Hacker's MachineThe CADR was the second-generation MIT Lisp Machine, the blueprint for commercial machines from Symbolics and LMI. Because its design files and microcode are publicly available, it is the most well-documented and emulated Lisp Machine.usim: The Microcode InterpreterThe primary emulator for the CADR is usim.Interpretation Strategy: usim operates at the microcode level. It interprets the 48-bit microinstructions that constitute the CADR's instruction set. It simulates the Unibus peripherals, disk drives, and the Chaosnet network interface.43Boot Process: The source code (tumbleweed.nu/r/usim) reveals the intricate boot sequence. It first loads a "prom" image (the initial microcode loader), which then loads the main microcode from a simulated disk, and finally boots the "macrocode" load band—the Lisp operating system itself.43Network Fidelity: A significant feature of usim is its support for Chaosnet, the pre-TCP/IP network protocol used at MIT. It includes a chaosd daemon that tunnels Chaosnet packets over modern UDP/IP, allowing emulated CADRs to communicate with each other and with host-based Chaosnet servers.445.2. TI Explorer and LMI Lambda: The Commercial ForksTexas Instruments and Lisp Machines Inc. (LMI) produced commercial derivatives of the MIT design, adding their own proprietary enhancements and hardware architectures (like NuBus).Nevermore: Lisp Emulating LispNevermore is an emulator for the TI Explorer I, distinguished by its implementation language: Common Lisp.45Meta-Circular Emulation: Writing a Lisp Machine emulator in Lisp offers a unique "meta-circular" aesthetic. The emulator simulates the Explorer's hardware and microcode using the high-level constructs of Common Lisp. While this approach is theoretically elegant, it faces performance challenges compared to C-based emulators like usim.Status: The project has successfully booted the Explorer microcode and Lisp environment but struggles with speed. It requires specific PROM images and disk images, which are often difficult to source due to bit-rot and copyright.46LambdaDelta: The LMI LambdaLambdaDelta emulates the LMI Lambda, a machine famous for its "SDU" (System Diagnosis Unit)—a separate 68000-based computer that controlled the Lisp processor.Full System Simulation: LambdaDelta simulates this multi-processor architecture, including the Nubus backplane. It is capable of bootstrapping the system from raw disk images and running the Lambda OS, providing a vital preservation link for this rare hardware.476. RISC and EPIC Workstations: The Unix Wars LegacyThe 1990s witnessed a Cambrian explosion of RISC architectures (PA-RISC, SPARC, MIPS) and the ambitious, ultimately doomed, experiment of EPIC (Itanium). These machines ran the commercial Unix variants (HP-UX, IRIX, Solaris) that powered the enterprise before the rise of Linux x86. Preserving these architectures is essential for maintaining the history of Unix.6.1. PA-RISC: HP 9000 EmulationThe Precision Architecture (PA-RISC) was HP's flagship RISC design, known for its segmented memory model and "Space Registers."QEMU: The Only Game in TownFor years, PA-RISC emulation was non-existent. However, support was added to QEMU in 2018, and it remains the only viable open-source emulator for this architecture.49Target Machines: QEMU focuses on emulating two specific workstations: the HP Visualize B160L (equipped with a 32-bit PA-7300LC CPU) and the HP Visualize C3700 (equipped with a 64-bit PA-8700 CPU).49 This selection covers both the 32-bit and 64-bit eras of the architecture.Operating System Support: The emulator is capable of booting HP-UX 11i, as well as the PA-RISC ports of Linux (Debian and Gentoo) and NextSTEP.Firmware (SeaBIOS): A critical challenge in emulating proprietary workstations is the firmware. Real PA-RISC machines use PDC (Processor Dependent Code). QEMU solves this by including a precompiled firmware based on a fork of SeaBIOS, which implements the necessary PDC calls to initialize the hardware and hand control over to the OS bootloader.50Technical capabilities: The emulation includes support for the PCI bus, the Dino PCI bridge, and the DEC 21142/43 "Tulip" network adapter, which was standard on these machines.496.2. Itanium (IA-64): The Explicitly Parallel ChallengeItanium (IA-64) was designed to replace RISC with VLIW (Very Long Instruction Word) concepts, relying on the compiler to manage instruction dependencies rather than the hardware. This complexity makes it notoriously difficult to emulate efficiently.Ski: The User-Mode SimulatorThe primary open-source tool for Itanium is Ski, an instruction set simulator originally developed by HP.51Scope Limitation: Ski is not a full system emulator in the same sense as QEMU. It was designed primarily for software development, allowing developers to test IA-64 user-space binaries on x86 machines. It "traps" system calls made by the guest application and handles them on the host OS.52System Emulation Gap: Ski lacks the full chipset emulation (IO-APIC, ACPI, PCI roots) required to boot a stock operating system kernel like Windows Server 2003 for Itanium or HP-UX. While it can run a Linux kernel in a specialized simulation mode, it cannot replace the hardware for preservation purposes.Status: The source code is available under the GPL-2 license and is maintained in various forks (e.g., github.com/trofi/ski), but active development has largely stalled. While there are periodic discussions about adding IA-64 support to QEMU, the architecture's complexity and lack of broad interest have prevented a complete implementation.537. Programmable Calculators: The Z80 and Saturn LegacyFor millions of engineers and scientists, their first "computer" was a programmable calculator from Texas Instruments or Hewlett-Packard. These devices utilized unique architectures and operating systems that are now robustly preserved through open-source emulation.7.1. TI Graphing Calculators (Z80/eZ80)The TI-83 and TI-84 Plus series are ubiquitous in education. Their architecture is based on the Zilog Z80, a standard 8-bit microprocessor, but with custom ASICs for display and I/O.TilEm: High Fidelity Z80 EmulationTilEm is an emulator for the entire Z80-based family (TI-73 through TI-86).Hardware Simulation: It emulates not just the Z80 CPU but also the specific ASICs that control the LCD timing and the link port. This allows it to support the "virtual link," enabling file transfers between the emulator and a PC (or another emulator instance) as if they were physical devices connected by a cable.54Source Code: The project is written in C and uses the GTK+ library for its interface. The source code structure separates the hardware emulation core from the GUI, facilitating ports to other platforms.55CEmu: The eZ80 EraThe newer TI-84 Plus CE uses the eZ80 processor, a faster, pipelined version of the Z80 that supports a 24-bit address mode.Developer-Centric: CEmu is explicitly designed for developers. It includes advanced debugging tools like a code stepper, memory viewer, and CPU state inspector.Architecture: The core emulation logic is written in C for performance and portability, while the GUI is built with C++ and Qt. This separation allows the core to be easily embedded in other tools or automated testing frameworks.567.2. HP Calculators (Saturn/ARM)HP calculators (specifically the HP 48, 49, and 50g series) are revered for their RPN (Reverse Polish Notation) entry system and the RPL programming language.Emu48: The Gold StandardEmu48 is the definitive emulator for this family.Saturn Architecture: The HP 48 series used the Saturn CPU, a proprietary 4-bit architecture designed by HP for low power consumption. Emu48's core manages this unique nibble-based architecture.The ARM Transition (Saturn-on-ARM): Interestingly, the later HP 49g+ and 50g calculators switched to a standard ARM processor. However, to maintain software compatibility, HP wrote a Saturn emulator that ran on the ARM chip. Emu48 handles this by emulating the ARM hardware, which in turn runs the original HP ROM's Saturn emulator. This results in a "layered" emulation: the PC emulates ARM, which emulates Saturn, which runs the user's RPL code.57Legal Aspects: The emulator itself is GPL, but it requires a ROM dump from a real calculator to function. The snippets highlight that these ROMs remain copyrighted by HP, and the emulator cannot be distributed with them.578. Conclusion: The Code RemainsThe audit of these repositories reveals a resilient and diverse ecosystem of preservation. The strategies employed vary wildly based on the target era:Cloud Era: The preservation strategy is Code Availability. The source code is the machine. We preserve the system by mirroring the git repositories of Moby, Kubernetes, and Firecracker. The challenge here is dependency management and the complexity of build systems.Hardware Era: The preservation strategy is Emulation. The source code is the map of the machine. We preserve the system by writing software (in C, C++, or even Lisp) that reads that map and simulates the territory. The challenge here is the retrieval of proprietary knowledge—ROM images, microcode, and ASIC documentation—and the implementation of timing-accurate logic.As we move further from the silicon of the 20th century, these emulation layers—from QEMU's PA-RISC translation to Emu48's nibble shuffling—become the only way to verify and experience our digital history. Simultaneously, the abstraction technologies of the present (Wasm, MicroVMs) are creating a future where hardware dependencies are minimized, perhaps making the preservation tasks of the next century slightly less arduous.9. Appendix: Repository & Artifact IndexThe following table serves as a quick-reference index for researchers to locate the primary source code and binaries discussed in this report.Architecture / TechnologyProject NamePrimary LanguageRepository / Source URLKey FunctionContainer RuntimeMoby (Docker)Gogithub.com/moby/mobyComponent library for container systems.Container EnginePodmanGogithub.com/containers/podmanDaemonless container engine.OrchestrationKubernetesGogithub.com/kubernetes/kubernetesContainer orchestration & scheduling.MicroVMFirecrackerRustgithub.com/firecracker-microvm/firecrackerServerless KVM micro-virtualization.User-space KernelgVisorGogithub.com/google/gvisorSecure sandbox intercepting syscalls.WebAssemblyWasmtimeRustgithub.com/bytecodealliance/wasmtimeJIT-style Wasm runtime (Cranelift).WebAssemblyWasmerRustgithub.com/wasmerio/wasmerUniversal Wasm runtime (Multiple backends).AndroidCuttlefishC++, Gogithub.com/google/android-cuttlefishVirtual Android device for AOSP.SymbianEKA2L1C++github.com/EKA2L1/EKA2L1HLE emulator for Symbian S60.PalmOSMuCgithub.com/meepingsnesroms/MuCycle-accurate PalmOS emulator.Lisp Machineusim (CADR)Ctumbleweed.nu/r/usimMIT CADR microcode interpreter.Lisp MachineNevermoreCommon Lispunlambda.com / github mirrorsTI Explorer emulator in Lisp.PA-RISCQEMUCgithub.com/qemu/qemuFull system emulation for HP B160L/C3700.ItaniumSkiCski.sourceforge.netIA-64 instruction set simulator.CalculatorsEmu48Chp.giesselink.com/emu48.htmHP 48/49/50g Saturn/ARM emulator.CalculatorsCEmuC, C++github.com/CE-Programming/CEmuTI-84 Plus CE (eZ80) emulator.ErlangBEAM (OTP)Cgithub.com/erlang/otpErlang Virtual Machine source.
